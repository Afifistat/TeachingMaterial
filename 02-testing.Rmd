---
title: "Statistics primer: Hypothesis testing"
author: "Laurent Gatto"
output:
    fig_caption: true
---

```{r, echo=FALSE}
source("make-fdr-diagram.R")
```
## Foreword

> the function of significance tests is to prevent you from making a
  fool of yourself, and not to make unpublishable results publishable
  [1]

at the end of the course, we will understand 

> Why Most Published Research Findings Are False. [3]

and will pledge to not make fools of ourselves too frequently.

## Statistical hypothesis testing

- Testing a working hypothesis

- Contrast an observed result to a *comparable* random distribution
  (assumptions!). If it is *different enough* from what we would
  expect by chance, then we might have an interesting result.

## The testing process

1. Research hypothesis, as defined by our [experimental
   design](https://github.com/lgatto/statistics-primer/blob/master/expdes-slides.pdf)

2. State the *null hypothesis* $H_0$ (no difference with random
   distribution) and *alternative hypothesis* $H_1$ (there is a difference
   with random distribution)

3. Consider statistical assumptions and decide which test in
   appropriate.

4. Perform test and carefully interpret results (see below)

## Choosing a test statistic

- Continuous data (such as micro-array, quantitative proteomic, ...)
  and we want to compare means: *t-test*

- Count data (high-throughput sequencing) and we want to compare the
  number of reads between two conditions: *negative binomial*

- Gene set enrichment (is there an enrichment of genes with a specific
  funtion in my set of interesting genes): *hypergeometric*

Check what is used in the literature!

## Type of tests: (non-)parametric

* Parametric: assumption is that the data comes from a population that
  follows a probability distribution

* Non-parametric: no defined/fixed parameters


When assumptions are correct, a parametric test has more *statistical
power*.

In practice, as most studies are under-powered, non-parametric tests
are not an option.
  
## Types of tests: One-sample or two-sample tests

```{r, echo=FALSE}
set.seed(1)
library(ggplot2)
library(gridExtra)
d <- data.frame(x = rnorm(15), Group = 1)
p1 <- ggplot(aes(y = x, x = Group), data = d) + geom_boxplot() + geom_jitter() + ylab("log2 fold-change") + geom_hline(yintercept = 0, colour = "red")

d <- data.frame(x = c(rnorm(15, 5), rnorm(15, 6.5)), Group = rep(LETTERS[1:2], each = 15))
p2 <- ggplot(aes(y = x, x = Group, colour = Group), data = d) +
      geom_boxplot() + geom_jitter() +
      theme(legend.position="none") 
grid.arrange(p1, p2, ncol = 2)
```

## Types of tests: paired tests

Measurements in two conditions are paired.

```{r, echo=FALSE}
set.seed(123)
d <- data.frame(y = c(sort(rnorm(15, 5)),
     		      sort(rnorm(15, 6.5))),
		x = c(jitter(rep(1, 15), amount = 0.15),
		      jitter(rep(2, 15), amount = 0.15)),
		Group = rep(1:2, each = 15))

dd <- cbind(d[1:15, 1:2], d[16:30, 1:3])
colnames(dd)[1:4] <- paste0(colnames(dd)[1:4], rep(1:2, each = 2))
dd$Group <- 1:15

p2 <- ggplot(aes(x = x, y = y, group = Group, colour = as.factor(Group)), data = d) +
             geom_boxplot() + geom_point() + theme(legend.position="none") +
	     xlab("Group") + ylab("Variable")
p2 <- p2 + scale_x_discrete("Group", limits = 1:2)
p2 + geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2), data = dd, colour = "grey")
```

## Working example: Student's t-test

Comparing means from 2 groups, continous data.

- $H_0: \mu_1 = \mu_2$
- $H_1: \mu_1 \neq \mu_2$

Assumptions:

- data is normally distributed
- data are independent and identically distributed
- equal or un-equal (Welch test) variance

- t-test is robust to deviations.

- (All models are wrong. Some are useful)

## Welch test (t-test with unequal variances)

Two samples of sizes $n_1$ and $n_2$

   $t = \frac{\bar{X_1} - \bar{X_2}}{s_{\bar{x_1} - \bar{x_2}}}$

where

  $s_{\bar{x_1} - \bar{x_2}} = \sqrt{\frac{s_{x1}^2}{n_1} + \frac{s_{x2}^2}{n_2}}$

## How to we estimate how different we are from *random*

```{r, echo = FALSE}
addseg <- function(x, ...) {
       segments(x, 0, x, dnorm(x), lty = "dotted", ...)
       segments(-1000, dnorm(x), x, dnorm(x), lty = "dotted", ...)
}
x <- rnorm(1e6)
plot(density(x), main = "N(0, 1)", xlab = "")
grid()
addseg(0.1, col = "black")
addseg(1, col = "blue")
addseg(2.3, col = "red")
```

**p-value**: how (un)likely it would be to observe a value as extreme or
more extreme under $H_0$.

## P-value

**p-value**: how (un)likely it would be to observe a value as extreme or
more extreme under $H_0$.

If p-value <= than (arbitrary) significance level $\alpha$, then we reject $H_0$.


## What can go wrong


|                |        H0 is true   |    H0 is false 
|----------------|---------------------|----------------	      
|H0 is rejected  | Type I error, FP    |   correct TP
|H0 not rejected |       correct TN    | Type II error, FN

power of a test = 1 - $type~II$

(FP: false postitive, TP: true positive, FN: false negative, TN: true positive)

**False discovery rate**: $\frac{FP}{FP+TP}$

## In R

```{r}
set.seed(1)
x <- rnorm(15, mean = 7, sd = 1)
y <- rnorm(15, mean = 5, sd = 1.2)
t.test(x, y)
```

## But

Is the p-value really what we want? What is the probability that we
make a fool of ourselves?

What about the power of our test? Let's calculate a false positive
rate.

##

```{r, echo=FALSE}
fdrdiagramme(sig.level = 0.01)
```

##

```{r, echo=FALSE}
fdrdiagramme(power = 0.5, preal = 0.05, sig.level = 0.05)
```

##

```{r, echo=FALSE}
fdrdiagramme(power = 0.5, preal = 0.9, sig.level = 0.05)
```

##

```{r, echo=FALSE}
preal <- seq(0.1, 0.9, 0.01)
sig <- c(0.05, 0.01)
power <- seq(0.1, 0.9, 0.01)
res <- expand.grid(preal = preal, sig = sig, power = power)
res$fdr <- apply(res, 1,
	   	 function(x)
		 fdrdiagramme(preal = x[1], sig.level = x[2], power = x[3], plot = FALSE))


## ggplot(aes(x = power, y = preal), data = res) + geom_tile(aes(fill = fdr)) + scale_fill_gradientn(colours = c("white", "red")) + facet_wrap(~sig)
ggplot(aes(x = power, y = preal), data = res) + geom_tile(aes(fill = fdr)) + scale_fill_gradientn(colours = c("white", "yellow", "orange", "red", "black")) + facet_wrap(~sig)
```

## Adjustment for multiple testing



## References


[1] David Colquhoun *An investigation of the false discovery rate and
    the misinterpretation of p-values* R. Soc. open sci. 2014 1
    140216;
    [doi:10.1098/rsos.140216](http://rsos.royalsocietypublishing.org/content/1/3/140216).

[2] Regina Nuzzo *Scientific method: Statistical errors* 2014 Nature
    506, 150â€“152
    [doi:10.1038/506150a](http://www.nature.com/news/scientific-method-statistical-errors-1.14700)

[3] Ioannidis JPA Why Most Published Research Findings Are False. 2015
    PLoS Med 2(8):
    e124. [doi:10.1371/journal.pmed.0020124](http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124).